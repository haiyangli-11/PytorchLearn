{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "general-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-shaft",
   "metadata": {},
   "source": [
    "torch.nn中封装了针对图像处理以及文本处理的多种神经网络处理单元，官方文档：https://pytorch.org/docs/stable/nn.html\n",
    "其中卷积层convd常见参数包含输入通道，输出通道，卷积核大小，步长等等。\n",
    "全连接层，在torch中抽象为矩阵乘法，使用`Liner`函数完成，假设输入特征矩阵为`b*n`，`Liner`函数中的参数矩阵为`m*b`，则代表输入特征数量为n， 输出特征数量为m，完成了一次特征降维。\n",
    "卷积操作特征输入输出大小关系：\n",
    "$H_{\\text {out }}=\\left\\lfloor\\frac{H_{\\text {in }}+2 \\times \\text { padding }[0]-\\text { dilation }[0] \\times(\\text { kernel_size }[0]-1)-1}{\\text { stride }[0]}+1\\right.$\n",
    "$W_{\\text {out }}=\\left\\lfloor\\frac{W_{\\text {in }}+2 \\times \\operatorname{padding}[1]-\\operatorname{dilation}[1] \\times(\\text { kernel_size }[1]-1)-1}{\\text { stride }[1]}+1\\right.$\n",
    "整个神经网络形状为：\n",
    "第一层：卷积层，输入通道1， 输出通道8，卷积核3*3\n",
    "第二层：卷积层，输入通道8， 输出通道16， 卷积核3\n",
    "第三层：全连接层，输入特征为图像大小*通道数量，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "upper-floating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 卷积层, conv函数位于torch.nn中的图像处理\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3)\n",
    "        self.fc1 = nn.Linear(16*6*6, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    # 前向计算函数\n",
    "    def forward(self, x):\n",
    "        # 卷积层计算\n",
    "        y = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        y = F.max_pool2d(F.relu(self.conv2(y)), (2,2))\n",
    "        # 将y形状从2维转换为1维\n",
    "        y = y.view(x.shape[0], -1)\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = F.softmax(self.fc3(y))\n",
    "        return y\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-colonial",
   "metadata": {},
   "source": [
    "检查神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "underlying-opening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([8, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-netherlands",
   "metadata": {},
   "source": [
    "生成随机输入，查看输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "functional-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.8900,  0.5631,  1.5987,  ..., -0.6284, -1.5422,  0.9578],\n",
      "          [-0.0208,  0.3048, -1.6220,  ...,  0.6611, -1.1264,  0.3424],\n",
      "          [ 0.4162, -0.1956,  0.9835,  ..., -1.1187,  0.1616, -0.4505],\n",
      "          ...,\n",
      "          [ 0.2703,  0.2097, -0.2778,  ..., -0.2481,  1.0752,  0.2217],\n",
      "          [ 1.3621,  0.4516,  1.8471,  ...,  2.2648,  0.4070, -0.8607],\n",
      "          [-0.6390,  0.5049,  0.2709,  ..., -0.0772,  0.8568,  0.6873]]]])\n",
      "tensor([[0.0882, 0.1065, 0.0994, 0.1091, 0.1099, 0.0894, 0.0931, 0.1028, 0.0994,\n",
      "         0.1021]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lihaiyang\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "testcase = torch.randn(1,1,32,32)\n",
    "print(testcase)\n",
    "out = net(testcase)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-jersey",
   "metadata": {},
   "source": [
    "清零梯度并随机初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "certain-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "directed-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5278, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "labels = torch.randn(1,10)\n",
    "lossF = nn.MSELoss()\n",
    "loss = lossF(out, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-garlic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fifteen-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 6.8219e-04, -9.3590e-04, -6.8058e-05, -3.0262e-04,  1.5181e-03,\n",
      "        -2.0048e-04,  5.6527e-04,  2.8303e-03])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward(retain_graph=True)\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "entitled-brush",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.1990, -0.2244, -0.1236, -0.2516,  0.2725, -0.1386, -0.0248, -0.0824],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1989, -0.2241, -0.1235, -0.2512,  0.2722, -0.1386, -0.0251, -0.0833],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lihaiyang\\.conda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "print(net.conv1.bias)\n",
    "optimzer = optim.SGD(net.parameters(), lr=0.5)\n",
    "optimzer.zero_grad()\n",
    "out = net(testcase)\n",
    "loss = lossF(labels, out)\n",
    "loss.backward()\n",
    "optimzer.step()\n",
    "print(net.conv1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-plaza",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
