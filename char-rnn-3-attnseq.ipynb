{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:30.770790Z",
     "start_time": "2021-04-04T05:01:30.362937Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:30.777698Z",
     "start_time": "2021-04-04T05:01:30.773112Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"\n",
    "    name:语言名称\n",
    "    index2word:标索引单词\n",
    "    word2index：单词索引下标\n",
    "    word2count：单词索引单次数量\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: 'SOS_token', 1: 'EOS_token'}\n",
    "        self.word2count = {}\n",
    "        self.n_word = 2\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index.keys():\n",
    "            self.index2word[self.n_word] = word\n",
    "            self.word2index[word] = self.n_word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_word += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:30.782899Z",
     "start_time": "2021-04-04T05:01:30.779790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode2ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode2ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:34.498213Z",
     "start_time": "2021-04-04T05:01:30.784485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading lines of language eng to fra\n",
      "[['go .', 'va !'], ['run !', 'cours !'], ['run !', 'courez !'], ['wow !', 'ca alors !'], ['fire !', 'au feu !']]\n"
     ]
    }
   ],
   "source": [
    "def readLang(lan1, lan2):\n",
    "    \"\"\"\n",
    "    读取文本文件，返回两个语言类Lang实例和对应的成对的语言列表\n",
    "    \"\"\"\n",
    "    print(\"reading lines of language %s to %s\" % (lan1, lan2))\n",
    "\n",
    "    with open('./data/eng-fra.txt', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        sentences = line.strip().split('\\t')\n",
    "        if len(sentences) < 2:\n",
    "            print(line)\n",
    "            print('-------error!------')\n",
    "            exit(-1)\n",
    "        pairs.append([\n",
    "            normalize_string(sentences[0].strip()),\n",
    "            normalize_string(sentences[1].strip())\n",
    "        ])\n",
    "    lang_src = Lang(lan1)\n",
    "    lang_dst = Lang(lan2)\n",
    "#     for pair in pairs:\n",
    "#         lang_src.add_sentence(pair[0])\n",
    "#         lang_dst.add_sentence(pair[1])\n",
    "\n",
    "    return lang_src, lang_dst, pairs\n",
    "\n",
    "\n",
    "lang_src, lang_dst, lang_pairs = readLang('eng', 'fra')\n",
    "print(lang_pairs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:34.502944Z",
     "start_time": "2021-04-04T05:01:34.499913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(lang_pairs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:34.621216Z",
     "start_time": "2021-04-04T05:01:34.504447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95170\n"
     ]
    }
   ],
   "source": [
    "MAX_WORD = 10\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [\n",
    "        pair for pair in pairs if len(pair[0].split(' ')) < MAX_WORD\n",
    "        and len(pair[1].split(' ')) < MAX_WORD\n",
    "    ]\n",
    "\n",
    "lang_pairs = filter_pairs(lang_pairs)\n",
    "print(len(lang_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:35.136434Z",
     "start_time": "2021-04-04T05:01:34.624257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng 10025\n",
      "fra 16813\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pair in lang_pairs:\n",
    "    lang_src.add_sentence(pair[0])\n",
    "    lang_dst.add_sentence(pair[1])\n",
    "\n",
    "print(lang_src.name, lang_src.n_word)\n",
    "print(lang_dst.name, lang_dst.n_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:35.142584Z",
     "start_time": "2021-04-04T05:01:35.139124Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRnn(nn.Module):\n",
    "    def __init__(self, dict_size, output_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dict_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_data, hidden):\n",
    "        \"\"\"\n",
    "        gru要求的输入形状为 seqlen*batch*dims，\n",
    "        此处输入为单个词的word2index索引，使用embedding转换为指定尾数的输入向量\n",
    "        input_data: [index] \n",
    "        hidden: (1*1*hidden_size), 1*1为本处指定，句子长度为1，batch为1\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_data).view(1, 1, -1)\n",
    "        output_data, hidden = self.gru(embedded, hidden)\n",
    "        return output_data, hidden\n",
    "\n",
    "    def init_hidden():\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:35.149078Z",
     "start_time": "2021-04-04T05:01:35.144048Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRnn(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, dict_size):\n",
    "        super(DecoderRnn, self).__init__()\n",
    "        self.embedding = nn.Embedding(dict_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, dict_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        embedded = self.embedding(input_data).view(1, 1, -1)\n",
    "        embedded = F.relu(embedded)\n",
    "        gru_out, gru_hidden = self.gru(embedded, hidden)\n",
    "        gru_out = self.out(gru_out)\n",
    "        output_data = self.softmax(gru_out[0])\n",
    "        return output_data, hidden\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:01:35.154548Z",
     "start_time": "2021-04-04T05:01:35.150569Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-10-1e096d7f8657>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-1e096d7f8657>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class DecoderAttnRnn(nn.Module):\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class DecoderAttnRnn(nn.Module):\n",
    "    def __init__(self, hidden_size, dict_size, drop = 0.1, maxlen = MAX_WORD):\n",
    "        super(DecoderAttnRnn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(dict_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.attn = nn.Linear(hidden_size + embedding_size, maxlen)\n",
    "        self.attn_cat_hidden = nn.Linear(embedding_size*2, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, dict_size)\n",
    "        \n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_data).view(1, 1, -1)\n",
    "        droped_embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_in = torch.cat((droped_embedded[0], hidden[0]), dim=1)\n",
    "        attn_weight = F.softmax(self.attn(attn_in), dim=1)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weight.unsqueeze(0)\n",
    "                                encoder_outputs.unsqueeze(0))\n",
    "        attn_cat_hidden = self.attn_cat_hidden(\n",
    "            torch.cat((attn_applied[0], hidden[0]), dim=1)\n",
    "        ).unsqueeze(0)\n",
    "        gru_in = F.relu(attn_cat_hidden)\n",
    "        out, hidden = self.gru(gru_in, hidden)\n",
    "        out = self.out(F.log_softmax(out[0], dim=1))\n",
    "        return out, hidden, attn_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 283,
   "position": {
    "height": "305px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
